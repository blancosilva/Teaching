%!TEX root = main.tex

\section{Sufficient Conditions}

It all boils down to a single result.

\begin{theorem}[KKT Sufficient Conditions]\label{theorem:KKTsufficient}\index{Theorem!KKT sufficient conditions}\index{Theorem!Karush-Kuhn-Tucker}
Let $\x \in S$ be a feasible point of the consistent program $(P)$ for which there are multipliers $u_k \geq 0$ ($1\leq k \leq m)$ and $v_k \in \field{R}$ ($1\leq k \leq \ell$) satisfying the conditions \ref{item:KKTnecessary1}, \ref{item:KKTnecessary2} and \ref{item:KKTnecessary3} of Theorem \ref{theorem:KKTnecessary}. If $f$ is pseudo-convex, $g_k$ is quasi-convex for all $1\leq k \leq m$, and $h_k$ is linear for all $1\leq k \leq \ell$, then $\x$ is a global optimal solution of $(P)$.
\end{theorem}

\begin{example}
We saw that the point $(0,0)$ satisfies the KKT conditions for the super-consistent convex program $(P)$ in Example \ref{example:feasibleP1}.  As a consequence of Theorems \ref{theorem:KKTnecessary} and \ref{theorem:KKTsufficient}, this point must be the optimal global minimum of $(P)$.

We also saw that the point $(2,1)$ satisfies the KKT conditions for the program $(P)$ in Example \ref{example:feasibleP3}.  It is not hard to see that this program is super-consistent, $f$ is pseudo-convex, $g_1$ and $g_2$ are quasi-convex, and $h_1$ is linear.  By virtue of Theorems \ref{theorem:KKTnecessary} and \ref{theorem:KKTsufficient}, the point $(2,1)$ must be the optimal solution of $(P)$.
\end{example}

\section*{Key Examples}
In the following section we are going to use the KKT conditions to address the characterization of optimal solutions of generic programs.  We start with one of the simplest settings.

\begin{example}
Let $\boldsymbol{A}$ be a symmetric $d \times d$ square matrix.  Consider the associated quadratic form $\quadratic{A}(\x)$.  We wish to find the global \emph{maximum} over all points of this function in the unit ball in $\field{R}^d$: $\field{B}_d = \{ \x \in \field{R}^d : \norm{\x} \leq 1 \}$.

An equivalent program $(P)$ is thus defined with $f(\x) = -\quadratic{A}(\x)$ as its objective function, and a single inequality constraint $g_1(\x) = \norm{\x}-1$.  This is a super-consistent program with a convex inequality constraint.  Checking the KKT conditions is justified under the hypothesis of Theorem \ref{theorem:Slater}.  Notice that 
\begin{align*}
\gradient{f}(\x) &= -2 \langle \boldsymbol{A}, \x \rangle, \\
\gradient{g_1}(\x) &= 2\x;
\end{align*}
therefore, the KKT conditions request the search for $\x \in \field{B}_d$ and $u \geq 0$ so that $u\big(1-\norm{\x}\big)=0$ and $-2\langle \boldsymbol{A}, \x \rangle + 2u\x = \boldsymbol{0}$.  

One obvious solution of the KKT conditions is $u=0$, $\x=\boldsymbol{0}$.  In this case, it is $f(\boldsymbol{0})=0$.

If we impose $u>0$, then it must be $\norm{x}=1$ by the first condition.  The second condition states that $\x$ must be an eigenvector of $\boldsymbol{A}$ with eigenvalue $u$: $\boldsymbol{A} \transpose{\x} = u \transpose{\x}$.  The value of the objective function in this case is $f(\x) = -\quadratic{A}(\x) = -\x \boldsymbol{A} \transpose{\x} = -\langle \x, u\x \rangle = -u \norm{\x}^2= -u$.  In order to obtain the requested global minimum value (different than zero), $u$ has to be the largest non-negative eigenvalue of $\boldsymbol{A}$ and $\x$ its corresponding normalized eigenvector.
\end{example}

\begin{example}
A simple case of the previous example: Set $\boldsymbol{A} = \big( \begin{smallmatrix} 1 & 3 \\ 3 & 1 \end{smallmatrix}\big)$.  The eigenvalues of $\boldsymbol{A}$ are $-2$ and $4$, and therefore the maximum of the associated quadratic form $\quadratic{A}(x,y) = x^2+y^2+6xy$ over the ball $x^2+y^2\leq 1$ happens at the (normalized) solution of the system
\begin{equation*}
\begin{bmatrix} 1 & 3 \\ 3 & 1 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} = 4\begin{bmatrix} x \\ y \end{bmatrix}.
\end{equation*}
This gives the points $\pm(\sqrt{2}/2, \sqrt{2}/2)$.
\end{example}