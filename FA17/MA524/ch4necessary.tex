%!TEX root = main.tex

\section{Necessary Conditions}
We begin with two results that focus on the structure of the equality constraints.
\begin{theorem}[Geometric Necessary Condition for Linear Equality Constraints]\index{Theorem!Geometric necessary condition}
If $(P)$ is a consistent program with linear equality constraints $h_k(\x) = \langle \boldsymbol{a}_k, \x \rangle + b_k$ ($\boldsymbol{a}_k \in \field{R}^d$, $b_k \in \field{R}$) for all $1 \leq k \leq \ell$, then for all feasible local minima $\x \in S$,
\begin{equation*}
\mathcal{F}_0(\x) \cap \mathcal{G}_0(\x) \cap \mathcal{H}_0(\x) = \emptyset.
\end{equation*}
\end{theorem}

\begin{theorem}[Geometric Necessary Condition for Linearly Independent Equality Constraints]\index{Theorem!Geometric necessary condition}
If $\x \in S$ is a feasible local minimum for the consistent program $(P)$, and the gradient vectors $\{ \gradient{h_k}(\x) : 1 \leq k \leq \ell \}$ are linearly independent, then $\mathcal{F}_0(\x) \cap \mathcal{G}_0(\x) \cap \mathcal{H}_0(\x) = \emptyset$.
\end{theorem}

\separator

As a consequence, an algebraic version of this geometric necessary condition gives the following result.

\begin{theorem}[Fritz John Necessary Conditions]\index{Theorem!Fritz John necessary conditions}\label{theorem:FritzJohn}
If $\x \in S$ is a feasible local minimum of the consistent program $(P)$, then there exist $\lambda_k \geq 0$ for $0\leq k \leq m$, and $\mu_1, \dots, \mu_\ell \in \field{R}$ so that
\begin{enumerate}
 	\item $[\lambda_0, \lambda_1, \dotsc, \lambda_m, \mu_1 \dotsc, \mu_\ell ] \neq \boldsymbol{0}$,
 	\item $\lambda_k g_k(\x) = 0$ for all $1 \leq k \leq m$.
 	\item $\lambda_0 \gradient{f}(\x) + \sum_{k=1}^m \lambda_k \gradient{g_k}(\x) + \sum_{k=1}^\ell \mu_k \gradient{h_k}(\x) = 0$.
 \end{enumerate}
\end{theorem}

\begin{example}
Continuing with example \ref{example:feasibleP1}, let's check if the point $(0,0)$ is a candidate to optimal solution of this program.  Let's use Theorem \ref{theorem:FritzJohn} to verify this claim:
\begin{align*}
\gradient{f}(x,y) &= [ 4x^3, 4y^3 ] &\gradient{f}(0,0) &= [0,0], \\
\gradient{g_1}(x,y) &= [ 2x, 0 ] &\gradient{g_1}(0,0) &= [0,0], \\
\gradient{g_2}(x,y) &= [0, 2y] &\gradient{g_2}(0,0) &= [0,0], \\
\gradient{g_3}(x,y) &= \big[e^{x+y}, e^{x+y} \big] &\gradient{g_3}(0,0) &= [1,1].
\end{align*}
Notice how the gradients \emph{line up} nicely---can we find $\lambda_k \geq 0$ so that:
\begin{enumerate}
	\item $[\lambda_0, \lambda_1, \lambda_2, \lambda_3] \neq [0,0,0,0]$,
	\item $\lambda_1=\lambda_2=0$ (since $\lambda_1 g_1(0,0) = -2\lambda_1$ and $\lambda_2 g_2(0,0) = -2\lambda_2$), and
	\item the following linear combination is equal to $[0,0]$
	\begin{gather*}
	\lambda_0\gradient{f}(0,0) + \lambda_1 \gradient{g_1}(0,0) + \lambda_2 \gradient{g_2}(0,0) + \lambda_3 \gradient{g_3}(0,0) = [0,0], \\
	\lambda_0[0,0] + \lambda_1[0,0] + \lambda_2[0,0] + \lambda_3[1,1] = [0,0],
	\end{gather*}
\end{enumerate}
We may select, for instance $\lambda_0=1$, $\lambda_1=\lambda_2=\lambda_3=0$, which proves that the point $(0,0)$ is indeed a candidate for optimal solution of $(P)$.
\end{example}

\separator
Further properties of the involved functions provide us with simpler sets of conditions

\begin{theorem}[Karush-Kuhn-Tucker Necessary Conditions]\index{Theorem!Karush-Kuhn-Tucker}\index{Theorem!KKT necessary conditions}\label{theorem:KKTnecessary}
If $\x \in S$ is a feasible local minimum of the consistent program $(P)$ for which all the vectors $\{ \gradient{h_k}(\x), \gradient{g_j}(\x) : 1 \leq k \leq \ell, j \in \mathcal{I}(\x) \}$ are linearly independent, then there exist $\lambda_k \geq 0$ for $1\leq k \leq m$, and $\mu_1, \dots, \mu_\ell \in \field{R}$ so that
\begin{enumerate}
 	\item\label{item:KKTnecessary1} $\lambda_k g_k(\x) = 0$ for all $1 \leq k \leq m$.
 	\item\label{item:KKTnecessary2} $\gradient{f}(\x) + \sum_{k=1}^m \lambda_k \gradient{g_k}(\x) + \sum_{k=1}^\ell \mu_k \gradient{h_k}(\x) = 0$.
 \end{enumerate}
\end{theorem}

\begin{remark}\index{KKT conditions}\index{Multipliers}
The conditions \ref{item:KKTnecessary1} and \ref{item:KKTnecessary2} of Theorem \ref{theorem:KKTnecessary} are called \emph{the KKT conditions} of the program $(P)$ in the literature.   The values $\lambda_k$, $\mu_k$ are called \emph{multipliers}.
\end{remark}

\begin{example}\label{example:feasibleP3}
Set $f(x,y)=(x-12)^2+(y+6)^2$.  Consider the program $(P)$ designed to find the global minimum of this function on the set $S=\{ (x,y) \in \field{R}^2 : x^2+3x+y^2-4.5y \leq 6.5, (x-9)^2 +y^2 \leq 64, 8x+4y=20 \}$.  We want to prove that the point $(2,1)$ is a good candidate for optimal solution of $(P)$.
The point $(2,1)$ is feasible.  To see this, set 
\begin{align*}
g_1(x,y) &=x^2+3x+y^2-4.5y-6.5, \\
g_2(x,y) &=(x-9)^2+y^2-64, \\
h_1(x,y) &=8x+4y-20,
\end{align*}
(or simpler equivalent constraints), and notice that
\begin{equation*}
g_1(2,1)=0, \quad g_2(2,1)=-14, \quad h_1(2,1)=0.
\end{equation*}
\begin{figure}[ht!]
\includegraphics[width=0.5\linewidth]{images/feasibleP3.png}
% \begin{python}
% import numpy as np, matplotlib.pyplot as plt
% # plt.xkcd();
% from matplotlib.patches import Circle
% from matplotlib.collections import PatchCollection

% fig, ax = plt.subplots()
% ax.set_aspect('equal')
% patches = [Circle((-3/2,9/4), np.sqrt(221/16)), Circle((9,0), 8)]
% p = PatchCollection(patches, alpha=0.4)
% ax.add_collection(p)
% plt.xlim(-7,20)
% plt.ylim(-10,15)
% plt.plot([-5,20],[15,-35])
% plt.plot(2,1,'r.')
% plt.plot([0,0],[-10,15],'k',alpha=0.4)
% plt.plot([-7,20],[0,0],'k', alpha=0.4)
% plt.title("The feasibility region\n is the intersection\n of these three objects")
% plt.show()
% \end{python}
\caption{Feasibility region for $(P)$ in example \ref{example:feasibleP3}}
\label{figure:feasibleP3}
\end{figure}
We have concluded that $(P)$ is consistent.  Notice that, $\mathcal{I}(2,1) = \{ 1 \}$, since $g_1(2,1) = 0$, $g_2(2,1) \neq 0$.  Further,
\begin{align*}
\gradient{f}(x,y)   &= [ 2(x-12), 2(y+6) ] &\gradient{f}(2,1)   &= [-20, 14], \\
\gradient{g_1}(x,y) &= [ 2x+3, 2y-4.5 ]    &\gradient{g_1}(2,1) &= [  7, -2.5], \\
\gradient{g_2}(x,y) &= [ 2(x-9), 2y ]      &\gradient{g_2}(2,1) &= [-14, 2], \\
\gradient{h_1}(x,y) &= [ 8, 4]             &\gradient{h_1}(2,1) &= [  8, 4], 
\end{align*}
The vectors $\gradient{g_1}(2,1) = [7,-2.5]$ and $\gradient{h_1}(2,1)=[8,4]$ are linearly independent.  Therefore, to verify that $(2,1)$ is candidate for optimal solution of $(P)$, we may now use Theorem \ref{theorem:KKTnecessary}.  The KKT conditions read as follows: we are looking for $\lambda_k \geq 0, \mu_1 \in \field{R}$ so that $\lambda_k g_k(2,1)=0$ ($k=1,2$) and
\begin{equation*}
\gradient{f}(2,1) + \lambda_1 \gradient{g_1}(2,1) + \lambda_2 \gradient{g_2}(2,1) + \mu_1 \gradient{h_1}(2,1) = [0,0], 
\end{equation*}
Let's address the first condition: Since $g_1(2,1)=0$ and $g_2(2,1)=-14<0$, it must be $\lambda_2=0$.  The second condition turns then into the equation
\begin{equation*}
[-20,14] + \lambda_1 [7,-2.5] + 0 \cdot [-14,2] + \mu_1 [8,4] = [0,0]
\end{equation*}
or equivalently
\begin{equation*}
\begin{bmatrix} 7 & 8 \\ -2.5 & 4 \end{bmatrix} \begin{bmatrix} \lambda_1 \\ \mu_1 \end{bmatrix} = \begin{bmatrix} 20 \\ -14 \end{bmatrix},
\end{equation*}
which gives $\lambda_1=4$, and $\mu_1=-1$.  This proves that the point $(2,1)$ is indeed a good candidate for the optimal solution of $(P)$.
\end{example}

\separator

There are other instances in which the KKT conditions can be used instead of those in the Fritz John Theorem.

\begin{theorem}[Slater Necessary Condition]\label{theorem:Slater}\index{Theorem!Slater condition}
Suppose that the inequality constraints $g_k$ of a super-consistent program $(P)$ are pseudo-convex ($1\leq k \leq m$), the equality constraints $h_k$ are linear ($1\leq k \leq \ell$), and the vectors $\gradient{h_k}(\x)$ are linearly independent at a feasible point $\x$.  Then the KKT conditions \ref{item:KKTnecessary1} and \ref{item:KKTnecessary2} of Theorem \ref{theorem:KKTnecessary} are necessary to characterize $\x$ as an optimal solution of $(P)$.
\end{theorem}

\begin{theorem}\label{theorem:KKTAllLinear}
If all constraints of a consistent program $(P)$ are linear, then the KKT conditions \ref{item:KKTnecessary1} and \ref{item:KKTnecessary2} of Theorem \ref{theorem:KKTnecessary} are necessary to characterize optimal solutions of $(P)$.
\end{theorem}