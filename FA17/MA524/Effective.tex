%!TEX root = main.tex

\section{Effective Algorithms for Unconstrained Optimization}
All of the methods we have explored so far (Newton-Raphson, Secant methods, Steepest descent) offer sound algorithms to compute local extrema of real-valued functions $f \colon \field{R}^d \to \field{R}$. They do have some pros and cons.  
\begin{itemize}
	\item The method of Steepest descent always converges to a local minimum, yet slowly.  The key is the fact that Steepest descent iterations are non-increasing.
	\item In order to obtain new approximations on each Steepest descent iteration, we have to solve many different one-dimensional optimizations, each of them offering their own computational issues. 
	\item Both Newton-Raphson and Secant methods offers faster sequences, but we cannot always guarantee convergence.  
	\item Another drawback of both Newton-Raphson and Steepest descent is the fact that we do need expressions for function itself, its gradient and Hessian matrix.  
	\item The recurrence formulas of Broyden's method are simple, and require only evaluations of the function itself.
\end{itemize}

The goal of this section is precisely gathering the best properties of the previous methods, so we may craft new methods with all the advantages, but none of the shortcomings.  

\separator

Given a function $f \colon \field{R}^d \to \field{R}$ with continuous first partial derivatives, and a given initial guess $\x_0 \in \field{R}^d$, we search for a recursive formula to approximate a minimum of $f$.  We request that this formula has the form
\begin{equation*}
\x_{n-1} = \x_n + t_n \w_n,
\end{equation*}
with positive parameters $t_n > 0$, and directions $\w_n$ (with $\norm{\w_n}=1$) satisfying the following two criteria:
\begin{description}
	\item[Non-increasing sequences] $f(\x_{n+1}) < f(\x_n)$ whenever $\gradient{f}(\x_n) \neq 0$. 

	\noindent This is achieved by requiring the direction $\omega_n$ to have an angle larger than $\pi/2$ with respect to the direction given by $\gradient{f}(\x_n)$:
	\begin{equation}
	\langle \w_n , \gradient{f}(\x_n) \rangle < 0.\label{Criterion2}
	\end{equation}
	Why does this work? Consider at each step $n \in \field{N}$ the restriction $\varphi_n(t)=f\big( \x_n + t \w_n \big)$ of $f$ over the line $\x_n + t \w_n$ with $t>0$.  We have then $\varphi_n'(0) = \langle \gradient{f}(\x_n), \w_n \rangle < 0$ (a decreasing function near $t=0$).  We have a guaranteed value $t_n > 0$ (that could be very small) that give us a point $\x_{n+1} = \x_n + t_n \w_n$ with $f(\x_{n+1}) < f(\x_n)$.
	\item[Control over length of steps] The steps $t_n$ are not \emph{too short, nor too long}.

	\noindent This is achieved by picking first $0 < \mu < \lambda < 1$ and forcing directions $\omega_n$ that satisfy
	\begin{gather}
	\langle \w_n , \gradient{f}(\x_{n+1}) - \lambda \gradient{f} (\x_n) \rangle > 0 \label{Criterion3}\\
	f(\x_{n+1}) \leq f(\x_n) + \mu t_n \langle \w_n \gradient{f}(\x_n) \rangle \label{Criterion4}
	\end{gather}
	if this is at all possible.
\end{description}

\separator

Is it possible to create a iteration satisfying these criteria?  The following result gives us a condition that helps in this regard:

\begin{theorem}[Wolfe]\label{theorem:Wolfe}\index{Theorem!Wolfe}
Suppose that $f \colon \field{R}^d \to \field{R}$ is a real-valued function with continuous partial derivatives.  Assume there exists $M \in \field{R}$ so that $f(x) \geq M$.  Let $\lambda, \mu$ be fixed numbers satisfying $0 < \lambda < \mu < 1$.  If $\w_n$, $\x_n \in \field{R}^d$ with $\norm{\w_n} = 1$ satisfy Criterion \eqref{Criterion2}, then there exist real numbers $a_n, b_n$ such that $0 \leq a_n < b_n$ and
\begin{enumerate}
	\item Criterion \eqref{Criterion3} is satisfied for any choice of $t_n \in (0, b_n)$, and
	\item Criterion \eqref{Criterion4} is satisfied for any choice of $t_n \in (a_n, b_n)$.
\end{enumerate}
\end{theorem}

\begin{remark}
For a proof, see \cite[Theorem 3.3.1]{peressini1988mathematics}.
\end{remark}


