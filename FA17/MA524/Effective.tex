%!TEX root = main.tex

\section{Effective Algorithms for Unconstrained Optimization}
All of the methods we have explored so far (Newton-Raphson, Secant methods, Steepest descent) offer sound algorithms to compute local extrema of real-valued functions $f \colon \field{R}^d \to \field{R}$. They do have some pros and cons.  
\begin{itemize}
	\item The method of Steepest descent always converges to a local minimum, yet slowly.  The key is the fact that Steepest descent iterations are non-increasing.
	\item In order to obtain new approximations on each Steepest descent iteration, we have to solve many different one-dimensional optimizations, each of them offering their own computational issues. 
	\item Both Newton-Raphson and Secant methods offers faster sequences, but we cannot always guarantee convergence.  
	\item Another drawback of both Newton-Raphson and Steepest descent is the fact that we do need expressions for function itself, its gradient and Hessian matrix.  
	\item The recurrence formulas of Broyden's method are simple, and require only evaluations of the function itself.
\end{itemize}

The goal of this section is precisely gathering the best properties of the previous methods, so we may craft new methods with all the advantages, but none of the shortcomings.  

\separator

Given a function $f \colon \field{R}^d \to \field{R}$ with continuous first partial derivatives, and a given initial guess $\x_0 \in \field{R}^d$, we search for a recursive formula to approximate a minimum of $f$.  We request that this formula has the form
\begin{equation*}
\x_{n-1} = \x_n + t_n \w_n,
\end{equation*}
with positive parameters $t_n > 0$, satisfying as many as possible from the following four criteria:
\begin{description}
	\item[Criterion 1] $f(\x_{n+1}) < f(x_n)$ whenever $\gradient{f}(\x_n) \neq 0$. 
	\item[Criterion 2] $\langle \w_n , \gradient{f}(\x_n) \rangle < 0$. 
	\item[Criterion 3] There exists $\lambda \in (0,1)$ so that for all $n\in \field{N}$,
	\begin{equation*}
	\langle \w_n , \gradient{f}(\x_{n+1}) - \lambda \gradient{f} (\x_n) \rangle > 0.
	\end{equation*}
	\item[Criterion 4] There exists $\mu \in (0,1)$, $\mu < \lambda$ so that for all $n \in \field{N}$,
	\begin{equation*}
	f(\x_{n+1}) \leq f(\x_n) + \mu t_n \langle \w_n \gradient{f}(\x_n) \rangle.
	\end{equation*}
\end{description}

\begin{remark} 
How do these criteria affect the behavior of the iterations and their convergence?
\begin{enumerate}
	\item Criterion 1 simply requests that, like in the method of Steepest descent, the iterations should be non-increasing.  This guarantees us convergence to an actual local minimum.  The problem with this criterion is that we are not given assurances that this could happen!  Criterion 2 helps with this task.
	\item Criterion 2 is stronger than Criterion 1.  Assume we have an iteration $\{ \x_n \}_{n \in \field{N}}$ satisfying this property.  Consider at each step $n \in \field{N}$ the restriction $\varphi(t)=f\big( \x_n + t \w_n \big)$ of $f$ over the line $\x_n + t \w_n$ with $t>0$.  We have then $\varphi'(0) = \langle \gradient{f}(\x_n), \w_n \rangle < 0$ (a decreasing function near $t=0$).  We have a guaranteed value $t_n > 0$ (that could be very small) that give us a point $\x_{n+1} = \x_n + t_n \w_n$ with $f(\x_{n+1}) < f(\x_n)$.  This is a somewhat constructive criterion, but it has a problem: the steps we take from one iteration to the next could be very small.  Is there a way to guarantee faster convergence?  Criterion 3 in tandem with Criterion 2 helps with this task.
	\item Assume that Criterion 2 is satisfied and we have found $t_n > 0$.  Given $0 < \lambda <1 $ that satisfies Criterion 3, 
\end{enumerate}
\end{remark}

\separator

Is it possible to create a iteration satisfying these criteria?  The following result gives us a condition that helps in this regard:

\begin{theorem}[Wolfe]\label{theorem:Wolfe}\index{Theorem!Wolfe}
Suppose that $f \colon \field{R}^d \to \field{R}$ is a real-valued function with continuous partial derivatives.  Assume there exists $M \in \field{R}$ so that $f(x) \geq M$.  Let $\lambda, \mu$ be fixed numbers satisfying $0 < \lambda < \mu < 1$.  If $\w_n$, $\x_n \in \field{R}^d$ satisfy 
\begin{equation*}
\langle \w_n, \gradient{f}(\x_n) \rangle < 0,
\end{equation*}  
then there exist real numbers $a_n, b_n$ such that $0 \leq a_n < b_n$ and
\begin{enumerate}
	\item Criterion 4 is satisfied for any choice of $t_n \in (0, b_n)$, and
	\item Criterion 3 is satisfied for any choice of $t_n \in (a_n, b_n)$.
\end{enumerate}
\end{theorem}

\begin{remark}
For a proof, see \cite[Theorem 3.3.1]{peressini1988mathematics}.
\end{remark}


