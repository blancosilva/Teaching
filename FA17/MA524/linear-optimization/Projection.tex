%!TEX root = main.tex

\section[Projection Methods]{Projection Methods for Linear Equality constrained programs}

Consider the minimization of a function $f\colon \field{R}^d \to \field{R}$ subject to $\ell$ linear constraints $h_k(\x) = 0$, where $h_k(\x) = \langle \boldsymbol{a}_k , \x \rangle - b_k$ for vectors $\boldsymbol{a}_k=[a_{k1}, \dotsc, a_{kd}] \in \field{R}^d$, and real values $b_k \in \field{R}$ ($1\leq k \leq \ell$).  If we set 
\begin{equation*}
\boldsymbol{A} = \begin{bmatrix} a_{11} & \dotsb & a_{1d} \\ \vdots & \ddots & \vdots \\ a_{\ell 1} & \dotsb & a_{\ell d} \end{bmatrix}
\end{equation*}
and $\boldsymbol{b} = [b_1, \dotsc, b_\ell]$ we may write these linear constraints as $\boldsymbol{A}\transpose{\x}=\transpose{\boldsymbol{b}}$.

The corresponding program (P) has $f$ as objective function and linear equality constraints $h_k\colon \field{R}^d \to \field{R}$ as defined above.  If this program happens to be consistent (it depends on $\boldsymbol{A}$ and $\boldsymbol{b}$), then Theorem \ref{theorem:KKTAllLinear} allows us to use KKT to find the optimal solutions. 

The KKT conditions read as follows: Find $\x \in \field{R}^d$ satisfying $\boldsymbol{A}\transpose{\x} = \transpose{\boldsymbol{b}}$, $\mu_k \in \field{R}$ for $1 \leq k \leq \ell$ so that
\begin{equation}\label{equation:KKTforLECP}
\gradient{f}(\x) + \sum_{k=1}^\ell \mu_k \boldsymbol{a}_k = \boldsymbol{0}.
\end{equation}


\begin{example}\label{example:ProjMethods}
We would like to find the minimum value of the function $f(x,y,z)=x^2+y^2+z^2$ over the line at the intersection of the planes $x+y+z=0$ and $x-y+2z=3$.  

The method we used in MATH 241 would start by computing a parameterization of the line first.  For instance, by forcing $z=0$ and solving the system formed by the two planes (with this restriction), we find that the point $(3/2,-3/2,0)$ belongs in this line.  The cross product of the normal vectors to the planes is the direction of the line: 
\begin{equation*}
[1,1,1] \times [1,-1,2] = \left\lvert \begin{matrix} \boldsymbol{i} & \boldsymbol{j} & \boldsymbol{k} \\ 1 & 1 & 1 \\ 1 & -1 & 2 \end{matrix} \right\rvert= [3,-1,-2].
\end{equation*}
We have then the line with equation $(3/2+3t, -3/2-t, -2t)$, $t\in \field{R}$.  A restriction of $f$ on this line gives
\begin{align*}
\varphi(t) &= f \big(\tfrac{3}{2}+3t, -\tfrac{3}{2}-t, -2t \big) \\
           &= \big( \tfrac{3}{2}  + 3t \big)^2 + \big( -\tfrac{3}{2} - t \big)^2 + (-2t)^2 \\
           &= \tfrac{9}{4} + 9t^2 + 9t + \tfrac{9}{4} + t^2 + 3t + 4t^2 \\
           &= 14t^2 + 12t + \tfrac{9}{2}
\end{align*}
The minimum of this function occurs at $t=-3/7$.  This yields the point 
\begin{equation*}
\big( \tfrac{3}{2} - 3 \cdot\tfrac{3}{7}, -\tfrac{3}{2} + \tfrac{3}{7}, 2\cdot \tfrac{3}{7} \big) = \big( \tfrac{3}{14}, -\tfrac{15}{14}, \tfrac{6}{7} \big).
\end{equation*}

The method we have explained in Chapter \ref{chapter:ConstrainedExistenceCharacterization} starts by collecting the constraints first, and claiming the use of the KKT conditions.  In this case, Theorem \ref{theorem:KKTAllLinear} guarantees we may use this technique.
\begin{align*}
h_1(x,y,z) &= x+y+z = \langle [1,1,1], [x,y,z]\rangle, \\
h_2(x,y,z) &= x-y+2z-3 = \langle [1,-1,2],[x,y,z] \rangle -3.
\end{align*}
The corresponding KKT condition request feasible points $(x,y,z) \in \field{R}^3$ and two real values $\mu_1, \mu_2 \in \field{R}$ that satisfy:
\begin{align*}
0 &= x+y+z, \\
3 &= x-y+2z, \\
[0,0,0] &= [2x,2y, 2z] + \mu_1 [1,1,1] + \mu_2 [1,-1,2].
\end{align*}
This gives the following system:
\begin{align*}
0 &= x+y+z = -\tfrac{1}{2}(\mu_1+\mu_2) -\tfrac{1}{2}(\mu_1-\mu_2) -\tfrac{1}{2}(\mu_1+2\mu_2), \\
3 &= x-y+2z = -\tfrac{1}{2}(\mu_1+\mu_2) + \tfrac{1}{2}(\mu_1-\mu_2) - (\mu_1+2\mu_2),
\end{align*}
which reduces to 
\begin{equation*}
\begin{bmatrix}  3/2 & 1 \\ 1 & 3 \end{bmatrix} \begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} = \begin{bmatrix} 0 \\ -3 \end{bmatrix}.
\end{equation*}
There is a unique solution $\mu_1 = 6/7$, $\mu_2 = -9/7$, with the point $\big( \tfrac{3}{14}, -\tfrac{15}{14}, \tfrac{6}{7} \big)$.
\end{example}

\separator

Notice how, in either case above, we ended solving the optimization problem \emph{symbolically}.  In this section we are going to adapt the techniques we learned in Chapter \ref{chapter:UnconstrainedNumerical} to approximate the solution of this kind of programs numerically.  As we have done in the past, it all starts by selecting a feasible initial guess, and solving a related program, associated to a simpler approximation instead.  We explore the following options:
\begin{itemize}
	\item Using linear approximations (this leads to the \emph{steepest des\-cent me\-thod}).
	\item Using quadratic approximations (this leads to the \emph{Newton method}).
\end{itemize}

\subsection{Steepest Descent}
Given a feasible initial guess $\x_0 \in \field{R}^d$ (satisfying $\boldsymbol{A}\transpose{\x_0}=\transpose{\boldsymbol{b}}$), we proceed to search for a next iteration $\bar{\x}_0$ \emph{within the feasibility region} that minimizes the linear approximant $L_0(\x) = f(\x_0) + \gradient{f}(\x_0)\transpose{(\x-\x_0)}$.  We usually further impose a maximum distance from $\x_0$ to $\bar{\x}_0$; that is, we force for example $\norm{\bar{\x}_0 - \x_0} \leq R_0$ for some $R_0>0$.  This gives the \emph{Direction Finding Program} $(DFP)$\index{Program!Direction Finding}
\begin{equation*}
\min_{\x \in S_0}L_0(\x), \quad S_0=\{ \x \in \field{R}^d :  \boldsymbol{A}\transpose{\x}=\transpose{\boldsymbol{b}}, \norm{\x-\x_0} \leq R_0 \}.
\end{equation*}
If we set $\v = \x - \x_0$, the program translates into simpler terms:
\begin{equation*}
\min_{\v \in S_0'} \gradient{f}(\x_0)\transpose{\v}, \quad S_0' = \{ \boldsymbol{A}\transpose{\v} = \transpose{\boldsymbol{0}}, \norm{\v} \leq R_0 \}.
\end{equation*}
Note this is a convex program (it is linear, as a matter of fact) with a Slater point at $\boldsymbol{0}$.  The KKT conditions for the $(DFP)$ program are therefore necessary and sufficient for optimality. 

Notice how much simpler these conditions are: Set $g_1(\v) = \norm{\v}^2-R_0^2$, $h_k(\v) = \langle \boldsymbol{a}_k, \v \rangle$ for $1\leq k \leq \ell$.  Find $\v \in \field{R}^d$ with $\boldsymbol{A}\transpose{\v} = \transpose{\boldsymbol{0}}$, $\lambda_1 \geq 0$, $\mu_k \in \field{R}$, $1\leq k \leq \ell$ so that:
\begin{gather*}
\lambda_1 \big( \norm{\v}^2 - R_0^2 \big) = 0, \\
\gradient{f}(\x_0) + 2\lambda_1 \v + \sum_{k=1}^\ell \mu_k \boldsymbol{a}_k = \boldsymbol{0}.
\end{gather*}
This gives two possibilities.
\begin{enumerate}
	\item $\lambda_1 = 0$, in which case we have $\gradient{f}(\x_0) + \sum_{k=1}^\ell \mu_k \boldsymbol{a}_k=0$.  This means that $\x_0$ is already a feasible point that satisfies the KKT conditions for $(P)$.
	\item $\lambda_1 \neq 0$, and $\v$ with $\norm{\v} = R_0$ that satisfies
	\begin{equation*}
	2\lambda_1 \v + \sum_{k=1}^\ell \mu_k \boldsymbol{a}_k = - \gradient{f}(\x_0).
	\end{equation*}
	A solution is found by the following formulas (it is easy to check why, and left to the reader):
	\begin{equation}\label{equation:projMethodsSteepest}
	\begin{split}
	\boldsymbol{P} = R_0^2 \big( \boldsymbol{I} - \transpose{\boldsymbol{A}} ( \boldsymbol{A} \transpose{\boldsymbol{A}} )^{-1} \boldsymbol{A} \big), \\
	\lambda_1 = \tfrac{1}{2} \quadratic{P}\big(\gradient{f}(\x_0)\big)^{1/2}, \\
	\transpose{[\mu_1, \dotsc, \mu_\ell]} = -(\boldsymbol{A}\transpose{\boldsymbol{A}})^{-1} \big( \boldsymbol{A}\transpose{\gradient{f}(\x_0)} \big), \\
	\transpose{\v} = -\quadratic{P}\big(\gradient{f}(\x_0)\big)^{-1/2} P \transpose{\gradient{f}(\x_0)}.
	\end{split}
	\end{equation}
	% If $\gradient{f}(\x_0)\transpose{\v}=\transpose{\boldsymbol{0}}$, then again $\x_0$ happens to be a feasible point satisfying the KKT conditions for $(P)$.
\end{enumerate}

The solution $\v_0$ of the $(DFP)$ points to a \emph{direction of steepest descent} for the program $(P)$ starting at $\x_0$.  We perform now a line-search\index{Line-search}: We proceed to restrict $f$ on the half line starting at $\x_0$ in the direction given by $\v_0$, and search for a global minimum:\index{Direction!of steepest descent}
\begin{equation*}
t_0 = \argmin_{t \geq 0} \varphi_0(t) = \argmin_{t \geq 0} f(\x_0 + t\v_0).
\end{equation*}
Set $\x_1 = \x_0 + t_0\v_0$.  We iterate this process to obtain a sequence of feasible points.  Given $\x_n \in \field{R}^d$ a feasible guess, and $R_n > 0$:
\begin{align*}
\v_n &= \argmin_{\v \in S_n'} \gradient{f}(\x_n)\transpose{\v}, \quad S_n' = \{ \v \in \field{R}^d : \norm{\v} \leq R_n, \boldsymbol{A}\transpose{\v}=\transpose{\boldsymbol{0}}\} \\
t_n &= \argmin_{t \geq 0} \varphi_n(t) = \argmin{t \geq 0} f(\x_n + t \v_n) \\
\x_{n+1} &= \x_n + t_n \v_n.
\end{align*}

\begin{example}
Let's illustrate this technique with the running example \ref{example:ProjMethods}.  Assume the initial guess is the feasible point $(3/2, -3/2, 0)$.  The corresponding $(DFP)$ for a direction search with unit vectors reads as follows:
\begin{equation*}
\min_{\v \in S'} \langle [3,-3,0], \v \rangle, \quad S' = \big\{ \v \in \field{R}^3 : \norm{\v}\leq 1, \big[ \begin{smallmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{smallmatrix}\big] \transpose{\v} = \transpose{\boldsymbol{0}} \big\}
\end{equation*}
The KKT conditions request a feasible $\v \in \field{R}^3$, $\lambda_1 \geq 0$, $\mu_1, \mu_2 \in \field{R}$ so that
\begin{gather*}
\lambda_1(\norm{\v}^2 - 1) = 0, \\
2\lambda_1 \v + \mu_1 [1,1,1] + \mu_2 [1,-1,2] = [-3,3,0].
\end{gather*}
The formulas in \eqref{equation:projMethodsSteepest} give
\begin{align*}
\boldsymbol{P} &= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 2 \end{bmatrix} \bigg( \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 2 \end{bmatrix} \bigg)^{-1} \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{bmatrix} \\ 
&= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 3 & 2 \\ 2 & 6 \end{bmatrix}^{-1} \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{bmatrix} \\
&= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 3/7 & -1/7 \\ -1/7 & 3/14 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{bmatrix} \\
&= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 2 \end{bmatrix} \begin{bmatrix} 2/7 & 4/7 & 1/7 \\ 1/14 & -5/14 & 2/7 \end{bmatrix} \\
&= \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} - \begin{bmatrix} 5/14 & 3/14 & 3/7 \\ -3/14 & 1/14 & 1/7 \\ -3/7 & 1/7 & 2/7 \end{bmatrix} = \begin{bmatrix} 9/14 & -3/14 & -3/7 \\ -3/14 & 1/14 & 1/7 \\ -3/7 & 1/7 & 2/7 \end{bmatrix} \\
\intertext{and thus,}
\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} &= \bigg( \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 1 & -1 \\ 1 & 2 \end{bmatrix} \bigg)^{-1} \bigg( \begin{bmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{bmatrix} \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} \bigg) \\
&= \begin{bmatrix} 3/7 & -1/7 \\ -1/7 & 3/14 \end{bmatrix} \begin{bmatrix} 0 \\ 6 \end{bmatrix} = \begin{bmatrix} -6/7 \\ 9/7 \end{bmatrix}, \\
\lambda_1 &= \frac{1}{2} \bigg( \begin{bmatrix} 3 & -3 & 0 \end{bmatrix} \begin{bmatrix} 9/14 & -3/14 & -3/7 \\ -3/14 & 1/14 & 1/7 \\ -3/7 & 1/7 & 2/7 \end{bmatrix} \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} \bigg)^{1/2} = \tfrac{3}{7}\sqrt{14} \\
\transpose{\v_0} &= -\tfrac{1}{12} \sqrt{14} \begin{bmatrix} 9/14 & -3/14 & -3/7 \\ -3/14 & 1/14 & 1/7 \\ -3/7 & 1/7 & 2/7 \end{bmatrix} \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} \\
&= -\tfrac{1}{12}\sqrt{14} \begin{bmatrix} 18/7 \\ -6/7 \\ -12/7 \end{bmatrix} = \tfrac{1}{14}\sqrt{14} \begin{bmatrix} -3 \\ 1 \\ 2  \end{bmatrix}
\end{align*}
Notice how $\v_0$ satisfies all required constraints, and $\norm{\v_0}=1$.  This vector is an optimal solution of $(DFP)$, and therefore a \emph{direction of steepest descent} for $(P)$ from the point $(3/2, -3/2, 0)$.

We perform now the line search from $\x_0$ in this direction:\index{Line-search}
\begin{gather*}
\varphi_0(t) = f(\x_0 + t\v_0) = t^2 - \tfrac{6}{7} \sqrt{14} \, t + \tfrac{9}{2} \\
t_0 = \argmin_{t\geq 0} \varphi_0(t) = \tfrac{3}{7}\sqrt{14}
\end{gather*} 
We have then
\begin{equation*}
\x_1 = \underbrace{\big( \tfrac{3}{2}, -\tfrac{3}{2}, 0)}_{\x_0} + \underbrace{\tfrac{3}{7}\sqrt{14}}_{t_0} \, \underbrace{ \tfrac{1}{14}\sqrt{14} \big[ -3, 1,  2  \big]}_{\v_0} = \big( \tfrac{3}{14}, -\tfrac{15}{14}, \tfrac{6}{7} \big),
\end{equation*}
which happens to be the optimal solution of the program $(P)$.
\end{example}

\subsection{Newton-Raphson}\index{Newton-Raphson method}
Given a feasible initial guess $\x_0 \in \field{R}^d$ (satisfying $\boldsymbol{A}\transpose{\x_0}=\transpose{\boldsymbol{b}}$), we proceed to search for a next iteration $\x_1$ \emph{within the feasibility region} that minimizes the quadratic approximant  
\begin{equation*}
Q_0(\x) = f(\x_0) + \langle \gradient{f}(\x_0), \x-\x_0 \rangle + \tfrac{1}{2} \quadratic{\Hess{f}(\x_0)}(\x-\x_0).
\end{equation*}   
No further imposition on the distance between $\x_1$ and $\x_0$ is needed at this point.  The corresponding associated program $(P')$ becomes
\begin{equation*}
\min_{\x\in S} Q_0(\x), \quad S = \{ \x \in \field{R}^d: \boldsymbol{A}\transpose{\x} = \transpose{\boldsymbol{b}} \}.
\end{equation*}
Notice that $\gradient{Q_0}(\x) = \gradient{f}(\x_0) + \langle \Hess{f}(\x_0), \x- \x_0 \rangle$.  Replacing $\v = \x -\x_0$, the KKT conditions for the program $(P')$ read as follows: Find $\mu_k \in \field{R}$ ($1\leq k \leq \ell)$ and $\v \in \field{R}^d$ satisfying $\boldsymbol{A} \transpose{\v} = \transpose{\boldsymbol{0}}$ so that 
\begin{equation*}
\Hess{f}(\x_0) \cdot \transpose{\v} + \sum_{k=1}^\ell \mu_k \transpose{\boldsymbol{a}_k} = -\transpose{\gradient{f}(\x_0)}.
\end{equation*}
If $\det\Hess{f}(\x_0) \neq 0$, the system has a unique solution.  To compute it, set first $\boldsymbol{H} = [\Hess{f}(\x_0)]^{-1}$.  The solution in this case can be written according to the following formulas:
\begin{equation}\label{equation:projMethodsNewton}
\begin{split}
\transpose{\v_0} &= \big[ \big( \boldsymbol{H} \transpose{\boldsymbol{A}} \big) \big( \boldsymbol{A} \boldsymbol{H} \transpose{\boldsymbol{A}} \big)^{-1} \big( \boldsymbol{A} \boldsymbol{H} \big) - \boldsymbol{H} \big] \cdot \transpose{\gradient{f}(\x_0)} \\
\transpose{[ \mu_1, \dotsc, \mu_\ell ]} &= - \big[ \big( \boldsymbol{A} \boldsymbol{H} \transpose{\boldsymbol{A}} \big)^{-1} \big( \boldsymbol{A} \boldsymbol{H}) \big] \cdot \transpose{\gradient{f}(\x_0)}
\end{split}
\end{equation}
The optimal solution $\v_0$ of this system points to a \emph{Newton-Raphson direction} for the program $(P)$ starting at $\x_0$.\index{Newton-Raphson method!direction}  Set then $\x_1 = \x_0 + \v_0$.  We iterate this process to find a sequence of feasible points.  Given $\x_n \in \field{R}^d$ a feasible guess,
\begin{equation*}
\x_{n+1} = \x_n + \underbrace{\argmin_{\boldsymbol{A}\transpose{\v} = \transpose{\boldsymbol{0}} } \big( f(\x_n) + \langle \gradient{f}(\x_n), \v \rangle + \tfrac{1}{2} \quadratic{\Hess{f}(\x_n)}(\v) \big) }_{\v_n}.
\end{equation*}

\begin{example}
Let's illustrate this technique with the running example \ref{example:ProjMethods}. Assume once again that the initial guess is the feasible point $(3/2, -3/2, 0)$. The corresponding program $(P')$ to search for the Newton-Raphson direction is
\begin{equation*}
\min_{\v \in S} \big( \tfrac{9}{2} + \langle [3,-3,0], \v \rangle + \v\transpose{\v} \big), \quad S = \big\{ \v \in \field{R}^3 : \big[ \begin{smallmatrix} 1 & 1 & 1 \\ 1 & -1 & 2 \end{smallmatrix}\big] \transpose{\v} = \transpose{\boldsymbol{0}} \big\}
\end{equation*}
The KKT conditions request multipliers $\mu_1, \mu_2 \in \field{R}$ and a feasible $\v \in \field{R}^3$ so that
\begin{equation*}
2\v + \mu_1 [1,1,1] + \mu_2 [1,-1,2] = [-3,3,0].
\end{equation*}
Since $\Hess{f}\big(\tfrac{3}{2},-\tfrac{3}{2},0\big) = 2 \boldsymbol{I}_3$ (non-singular), we have $\boldsymbol{H} = \tfrac{1}{2} \boldsymbol{I}_3$.  The formulas in \eqref{equation:projMethodsNewton} give
\begin{align*}
\transpose{\v_0} &= \left[ \begin{bmatrix} 1/2 & 1/2 \\ 1/2 & -1/2 \\ 1/2 & 1 \end{bmatrix} \begin{bmatrix} 3/2 & 1 \\ 1 & 3 \end{bmatrix}^{-1} \begin{bmatrix} 1/2 & 1/2 & 1/2 \\ 1/2 & -1/2 & 1 \end{bmatrix} - \tfrac{1}{2} \boldsymbol{I}_3 \right] \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} \\
&= \begin{bmatrix} -9/28 & 3/28 & 3/14 \\ -3/28 & -1/28 & -1/14 \\ 3/14 & -1/14 & -1/7 \end{bmatrix} \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} = \begin{bmatrix} -9/7 \\ 3/7 \\ 6/7 \end{bmatrix} \\
\begin{bmatrix} \mu_1 \\ \mu_2 \end{bmatrix} &= - \begin{bmatrix} 3/2 & 1 \\ 1 & 3 \end{bmatrix}^{-1} \begin{bmatrix} 1/2 & 1/2 & 1/2 \\ 1/2 & -1/2 & 1 \end{bmatrix} \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} \\
&= - \begin{bmatrix} 2/7 & 4/7 & 1/7 \\ 1/14 & -5/14 & 2/7 \end{bmatrix} \begin{bmatrix} 3 \\ -3 \\ 0 \end{bmatrix} = \begin{bmatrix} 6/7 \\ -9/7 \end{bmatrix}
\end{align*}
At this point, we may calculate 
\begin{equation*}
\x_1 = \x_0 + \v_0 = \big( \tfrac{3}{2}, -\tfrac{3}{2}, 0 \big) + \big[ -\tfrac{9}{7}, \tfrac{3}{7}, \tfrac{6}{7} \big] = \big( \tfrac{3}{14}, -\tfrac{15}{14}, \tfrac{6}{7} \big),
\end{equation*}
which happens to be the optimal solution of the program $(P)$.
\end{example}